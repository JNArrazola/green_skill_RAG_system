{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b574dfb",
   "metadata": {},
   "source": [
    "# Green Job Detection and Normalization\n",
    "This notebooks aims to **detect** (know if a job contains green skills or not) and normalize (map the green skills from the *.csv* to the `ESCO` taxonomy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6eaf3ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import json \n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import jsonschema\n",
    "from pydantic import BaseModel\n",
    "import threading\n",
    "from threading import Lock\n",
    "\n",
    "from warning_management import send_warning\n",
    "from warning_management import send_error\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bfc738",
   "metadata": {},
   "source": [
    "## Loading the indexes and mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d23bc6",
   "metadata": {},
   "source": [
    "Load indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b966b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load indexes\n",
    "\"\"\"\n",
    "@Returns:\n",
    "    index_green_skills: FAISS index for green skills embeddings.\n",
    "    id_to_skill: Dictionary mapping skill IDs to skill names.\n",
    "    index_job_skills: FAISS index for job skills embeddings.\n",
    "    id_to_job: Dictionary mapping job IDs to job names.\n",
    "\"\"\"\n",
    "def get_indexes_and_mappings() -> tuple[faiss.Index, dict[str, str], faiss.Index, dict[str, str]]:\n",
    "    index_green_skills = faiss.read_index(\"../data/embeddings/esco_green_skills_text-embedding-3-large.index\")\n",
    "    index_job_skills = faiss.read_index(\"../data/embeddings/full_job_skills_embeddings.index\")\n",
    "\n",
    "    # Load mappings\n",
    "    id_to_job = json.load(open(\"../data/mapping/id_to_jobs.json\", \"r\"))\n",
    "    id_to_skill = json.load(open(\"../data/mapping/id_to_skills.json\", \"r\"))\n",
    "    return index_green_skills, id_to_skill, index_job_skills, id_to_job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecf7147",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_green_skills, id_to_skill, index_job_skills, id_to_job = get_indexes_and_mappings()\n",
    "\n",
    "a, b = (index_green_skills.ntotal, index_green_skills.d,)\n",
    "c, d = (index_job_skills.ntotal, index_job_skills.d,)\n",
    "\n",
    "print(f\"Green skills index contains {a} vectors of dimension {b}.\")\n",
    "print(f\"Job skills index contains {c} vectors of dimension {d}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17cd096a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset shape: (204372, 6)\n"
     ]
    }
   ],
   "source": [
    "df_jobs = pd.read_csv(\"../data/full_dataset/jul24_to_jul_2025_cleaned_sorted.csv\")\n",
    "print(\"Full dataset shape:\", df_jobs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0166f8",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be4a63e",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "- `K_N`: number of nearest neighbors to retrieve from the index.\n",
    "- `MIN_THRESHOLD`: minimum score threshold to consider an entry relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f41441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify this value in case of needing a different threshold for the \n",
    "# confidence of the green job detection\n",
    "MIN_THRESHOLD = 0.1\n",
    "\n",
    "# Modify this value in case of needing a different number of neighbors\n",
    "K_N = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c701f48",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "## Green Skill Classification Prompt\n",
    "We will use the following prompt to detect and normalize green skills in job descriptions, it receives as arguments: \n",
    "- `k_closest`: a list of the `k` closest green skills from the `ESCO` taxonomy.\n",
    "- `skill`: the skill extracted from the job description, and also the one we want to normalize.\n",
    "- `job`: the name of the job.\n",
    "It returns a list of dictionaries representing the messages to be sent to the model and the roles of each message.\n",
    "* **system**: It is a system message that defines the role of the model.\n",
    "* **user**: It is the user message that contains the actual prompt with the context and instructions.\n",
    "* **developer**: It is the developer message that contains the instructions for the model's response format (in this case the pydantic class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56babf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_green_skill_classification_prompt(k_closest: list[str], skill: str, job: str) -> list[dict]:\n",
    "    system_prompt = \"\"\"\n",
    "You are an expert in identifying whether a skill can be mapped to a *green skill* in the ESCO taxonomy.\n",
    "A green skill is defined as the abilities, values and attitudes needed to live in, develop and support\n",
    "a society which reduces the impact of human activity on the environment.\n",
    "\n",
    "See if the provided skill can be used to perform tasks that contribute to environmental sustainability.\n",
    "Examples: \n",
    "- knowledge of vehicle cleaning standards and processes -> perform cleaning activities in an environmentally friendly way\n",
    "- supervision de limpieza y preparacion de unidades -> perform cleaning activities in an environmentally friendly way\n",
    "- experiencia en reparaciones y mantenimiento de unidades -> maintain concentrated solar power systems\n",
    "- experiencia en pintura automotriz -> use environmental friendly materials\n",
    "- control de procesos -> monitor manufacturing impact\n",
    "- experiencia en proceso de gestion de inventario y ventas en el area de refacciones -> monitor ingredient storage\n",
    "- conocimiento en logistica -> develop efficiency plans for logistics operations\n",
    "\n",
    "Meaning that the skill can be used to carry out tasks that have a positive impact on the environment.\n",
    "\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "Determine if the following skill can be semantically matched to one of the provided green skills from ESCO.\n",
    "\n",
    "Skill to classify: \"{skill}\"\n",
    "Job context: \"{job}\"\n",
    "\n",
    "Closest ESCO green skills (formatted as 'Main Name': 'Alternative Name'):\n",
    "\"\"\"\n",
    "\n",
    "    for k in k_closest:\n",
    "        user_prompt += f\"- {k}\\n\"\n",
    "\n",
    "    user_prompt += \"\"\"\n",
    "Decide whether the skill can be reasonably mapped to one of the ESCO green skills above\n",
    "based on meaning, context and if it contributes to environmental sustainability in any way.\n",
    "\"\"\"\n",
    "\n",
    "    developer_prompt = \"\"\"\n",
    "You must respond strictly following the response schema.\n",
    "If the skill matches one of the ESCO green skills, return its 'Main Name'.\n",
    "Otherwise, return 'No'.\n",
    "\"\"\"\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
    "        {\"role\": \"user\", \"content\": user_prompt.strip()},\n",
    "        {\"role\": \"developer\", \"content\": developer_prompt.strip()}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2e47fc",
   "metadata": {},
   "source": [
    "# Models for output parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56235158",
   "metadata": {},
   "source": [
    "## GreenSkillClassification\n",
    "Model to parse the output of the green skill normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4a0c9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreenSkillClassification(BaseModel):\n",
    "    mapped_skill: str # The main name of the mapped ESCO green skill, or \"No\" if there is no match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5decbfa",
   "metadata": {},
   "source": [
    "## Model\n",
    "We will use the **gpt-4o** model from **OpenAI** for the normalization task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a7d7971",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' def createClient() -> OpenAI:\\n    return OpenAI (api_key=OPENAI_KEY) '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Load OpenAI key\n",
    "API_KEY = os.getenv(\"API_KEY\")\n",
    "ENDPOINT = os.getenv(\"ENDPOINT\")\n",
    "DEPLOYMENT = os.getenv(\"DEPLOYMENT\")    \n",
    "API_VERSION = os.getenv(\"API_VERSION\")\n",
    "\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_KEY\")\n",
    "\n",
    "# Source: https://platform.openai.com/docs/guides/structured-outputs\n",
    "# Function to run the model with output parsing\n",
    "# Load OpenAI key\n",
    "\n",
    "\"\"\" def run_model(prompt: list[str], client: OpenAI) -> str:\n",
    "    response = client.responses.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        input=[\n",
    "            {\"role\": msg[\"role\"], \"content\": msg[\"content\"]} for msg in prompt\n",
    "        ],\n",
    "        text_format=GreenSkillClassification,\n",
    "    )\n",
    "    return response.output_parsed.mapped_skill \"\"\"\n",
    "\n",
    "\n",
    "def run_model(prompt: list[str], client: OpenAI) -> str:\n",
    "    response = client.chat.completions.create(\n",
    "        model=DEPLOYMENT,  \n",
    "        messages=[\n",
    "            {\"role\": msg[\"role\"], \"content\": msg[\"content\"]} for msg in prompt\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "def createClient() -> OpenAI:\n",
    "    return OpenAI(\n",
    "        base_url=f\"{ENDPOINT}openai/deployments/{DEPLOYMENT}/\",\n",
    "        api_key=API_KEY,\n",
    "        default_query={\"api-version\": API_VERSION},\n",
    "        default_headers={\"api-key\": API_KEY},\n",
    "    )\n",
    "\n",
    "\n",
    "\"\"\" def createClient() -> OpenAI:\n",
    "    return OpenAI (api_key=OPENAI_KEY) \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70f252c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test run of the model\n",
    "\n",
    "D, I = index_green_skills.search(np.array([index_job_skills.reconstruct(0)]), K_N)\n",
    "k_closest = [f\"\\t{id_to_skill[str(idx)][0]}: {id_to_skill[str(idx)][1]}\" for idx in I[0]]\n",
    "name  = df_jobs[df_jobs[\"Job_ID\"] == id_to_job[str(0)][0]][\"Title\"].values[0]\n",
    "skill = id_to_job[str(0)][1]\n",
    "prompt = get_green_skill_classification_prompt(k_closest, skill, name)\n",
    "response = run_model(prompt, createClient())\n",
    "print(\"Model response:\", response)\n",
    "print(type(response))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a72fae4",
   "metadata": {},
   "source": [
    "## Prompt generator examples\n",
    "An entry used to generate sample prompts to test the prompt generator function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8b23d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "FROM, TO = 0, 100\n",
    "\n",
    "with open(\"../test/example_prompts.txt\", \"w\") as f:\n",
    "    index_green_skills, id_to_skill, index_job_skills, id_to_job = get_indexes_and_mappings()\n",
    "    for i in range(FROM, TO):\n",
    "        D, I = index_green_skills.search(np.array([index_job_skills.reconstruct(i)]), K_N)\n",
    "    \n",
    "        k_closest = [f\"\\t{id_to_skill[str(idx)][0]}: {id_to_skill[str(idx)][1]}\" for idx in I[0]]\n",
    "        name  = df_jobs[df_jobs[\"Job_ID\"] == id_to_job[str(i)][0]][\"Title\"].values[0]\n",
    "        \n",
    "        skill = id_to_job[str(i)][1]\n",
    "        prompt = get_green_skill_classification_prompt(k_closest, skill, name)\n",
    "        for msg in prompt:\n",
    "            f.write(f\"{msg['role'].upper()}:\\n{msg['content']}\\n\\n\")\n",
    "        f.write(\"-----\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824f1e98",
   "metadata": {},
   "source": [
    "## Normalization process\n",
    "The following code cell shows how the normalization process is done for every job skill in the dataset. The steps are:\n",
    "1. For every i-th job, get the top `K_N` nearest neighbors to the i-th job skill, comparing them with the `ESCO` green skill taxonomy using **cosine similarity** as distance metric.\n",
    "2. If the **max score** among the `K_N` neighbors is greater or equal than `MIN_THRESHOLD`, then we consider that the job **may** contains green skills, and we proceed to normalize it using the prompt defined before and the language model.\n",
    "3. We run the prompt, were we could get two types of answers:\n",
    "   - The skill has been normalized to one of the `k` closest green skills, in that case it returns the **normalized skill**.\n",
    "   - The skill could not be normalized to any of the `k` closest green skills, in that case it returns **No**.\n",
    "4. We save the entry in a new dataframe, and also keep a set to avoid duplicates (i.e. the same job skill being normalized to the same green skill more than once, which would lead to counting it multiple times when analyzing the results).\n",
    "\n",
    "In this case, we have to use **threads** in order to speed up the process, since we have to process a lot of entries and the model calls are time consuming.\n",
    "\n",
    "At first, every entry was taking around **20 seconds**, but with threads we were able to reduce it to around **2-3 seconds** per entry, which is a significant improvement.\n",
    "> WARNING: 20 threads crashed my system due to high memory usage (Ryzen 7 3800x, 32GB RAM), so I had to reduce it to 10 threads. Be careful when setting this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14ed8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"  \n",
    "Main task function to process a partition of job skills, classify them using the language model,\n",
    "and save the results to a CSV file.\n",
    "@Params:\n",
    "    left: Left index of the partition (inclusive).\n",
    "    right: Right index of the partition (exclusive).\n",
    "    partition_id: Identifier for the partition (used in the output file name).\n",
    "    global_set: A shared set to track already processed (job_id, skill_id) pairs which prevent overcounting.\n",
    "    save_every: Number of processed entries after which to save intermediate results. (default is 50)\n",
    "\"\"\"\n",
    "\n",
    "def write_log_string(message: str, ERROR_FILE_NAME: str) -> str:\n",
    "    with open(ERROR_FILE_NAME, \"a\") as f:\n",
    "        f.write(message + \"\\n\")\n",
    "    return message\n",
    "\n",
    "def task(left : int, right : int, partition_id: int, global_set: set[str], save_every: int = 50):\n",
    "    FILE_NAME = f\"../data/green_skill_classification/full_green_skills_with_GPT-4_part_{partition_id}.csv\"\n",
    "    ERROR_FILE_NAME = f\"../data/logs/green_skill_classification/errors_part_{partition_id}.log\"\n",
    "    CLIENT = createClient()\n",
    "\n",
    "    # Use a python array here because it was faster than a pandas dataframe for appending\n",
    "    df_new_dataset = []\n",
    "\n",
    "    # Every threads gets their own indexes and mappings in order to avoid conflicts\n",
    "    index_green_skills, id_to_skill, index_job_skills, id_to_job = get_indexes_and_mappings()\n",
    "\n",
    "    ctr = 0\n",
    "    for i in range(left, right):\n",
    "        try:\n",
    "            # Search for the k nearest green skills for the job skill at index i\n",
    "            D, I = index_green_skills.search(np.array([index_job_skills.reconstruct(i)]), K_N)\n",
    "        except Exception as e:\n",
    "            send_error(f\"Searching embeddings at index {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Check if the maximum similarity score is below the minimum threshold\n",
    "            # In that case, we directly add a \"No\" entry for this job skill, since it is not similar enough to any green skill\n",
    "            if max(D[0]) < MIN_THRESHOLD:\n",
    "                df_new_dataset.append({\n",
    "                    \"job_id\": id_to_job[str(i)][0],\n",
    "                    \"job_skill\": id_to_job[str(i)][1],\n",
    "                    \"month\": id_to_job[str(i)][2],\n",
    "                    \"year\": id_to_job[str(i)][3],\n",
    "                    \"skill_id\": \"No\",\n",
    "                    \"esco_skill_name\": \"No\",\n",
    "                    \"alternative_name\": \"No\",\n",
    "                    \"prompt\": \"No\"})\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            send_error(f\"Threshold check or concatenation failed at index {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Iterate through the k closest green skills and classify using the language model\n",
    "        flag = False # Flag to indicate if a mapping was found\n",
    "        try:\n",
    "            # Iterate through the k closest green skills (score, idx)\n",
    "            for score, idx in zip(D[0], I[0]):\n",
    "                try:\n",
    "                    name  = df_jobs[df_jobs[\"Job_ID\"] == id_to_job[str(i)][0]][\"Title\"].values[0]\n",
    "                    prompt = get_green_skill_classification_prompt([f\"{id_to_skill[str(idx)][0]}: {id_to_skill[str(idx)][1]}\" for idx in I[0]], id_to_job[str(i)][1], name)\n",
    "                    response = run_model(prompt, CLIENT)\n",
    "                except Exception as e:\n",
    "                    write_log_string(f\"Cannot generate prompt or run model at job index {i}, skill index {idx}: {e}\", ERROR_FILE_NAME)\n",
    "\n",
    "                    df_new_dataset.append({\n",
    "                        \"job_id\": id_to_job[str(i)][0],\n",
    "                        \"job_skill\": id_to_job[str(i)][1],\n",
    "                        \"month\": id_to_job[str(i)][2],\n",
    "                        \"year\": id_to_job[str(i)][3],\n",
    "                        \"skill_id\": \"No\",\n",
    "                        \"esco_skill_name\": \"No\",\n",
    "                        \"alternative_name\": \"No\",\n",
    "                        \"prompt\": \"No\"})\n",
    "                    break;\n",
    "\n",
    "                try:\n",
    "                    if response != \"No\" and (id_to_job[str(i)][0], id_to_skill[str(idx)][0]) not in global_set:\n",
    "                        new_prompt = str(prompt) + \" | \" + response\n",
    "                        new_prompt = new_prompt.replace(\"\\n\", \" \").replace(\"\\t\", \" \").strip()\n",
    "\n",
    "                        df_new_dataset.append({\n",
    "                            \"job_id\": id_to_job[str(i)][0],\n",
    "                            \"job_skill\": id_to_job[str(i)][1],\n",
    "                            \"month\": id_to_job[str(i)][2],\n",
    "                            \"year\": id_to_job[str(i)][3],\n",
    "                            \"skill_id\": list(id_to_skill.keys())[idx],\n",
    "                            \"esco_skill_name\": id_to_skill[str(idx)][0],\n",
    "                            \"alternative_name\": id_to_skill[str(idx)][1],\n",
    "                            \"prompt\": new_prompt})\n",
    "                        with lock:\n",
    "                            global_set.add((id_to_job[str(i)][0], id_to_skill[str(idx)][0]))\n",
    "                        flag = True\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    send_error(f\"Adding green skill at job index {i}, skill index {idx}: {e}\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            send_error(f\"Iterating through scores at index {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if not flag and (id_to_job[str(i)][1], \"No\") not in global_set:                \n",
    "                new_prompt = str(prompt) + \" | \" + response\n",
    "                new_prompt = new_prompt.replace(\"\\n\", \" \").replace(\"\\t\", \" \").strip()\n",
    "                df_new_dataset.append({\n",
    "                    \"job_id\": id_to_job[str(i)][0],\n",
    "                    \"job_skill\": id_to_job[str(i)][1],\n",
    "                    \"month\": id_to_job[str(i)][2],\n",
    "                    \"year\": id_to_job[str(i)][3],\n",
    "                    \"skill_id\": \"No\",\n",
    "                    \"esco_skill_name\": \"No\",\n",
    "                    \"alternative_name\": \"No\",\n",
    "                    \"prompt\": new_prompt})\n",
    "                with lock:\n",
    "                    global_set.add((id_to_job[str(i)][0], \"No\"))\n",
    "        except Exception as e:\n",
    "            df_new_dataset.append({\n",
    "                    \"job_id\": id_to_job[str(i)][0],\n",
    "                    \"job_skill\": id_to_job[str(i)][1],\n",
    "                    \"month\": id_to_job[str(i)][2],\n",
    "                    \"year\": id_to_job[str(i)][3],\n",
    "                    \"skill_id\": \"No\",\n",
    "                    \"esco_skill_name\": \"No\",\n",
    "                    \"alternative_name\": \"No\",\n",
    "                    \"prompt\": \"No\"})\n",
    "            continue\n",
    "        ctr += 1\n",
    "        if ctr % save_every == 0:\n",
    "            pd.DataFrame(df_new_dataset).to_csv(FILE_NAME, index=False)\n",
    "        with lock:\n",
    "            pbar.update(1)\n",
    "    pd.DataFrame(df_new_dataset).to_csv(FILE_NAME, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e9283b",
   "metadata": {},
   "source": [
    "Modified task for indexed task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50e7a0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_task(index_list: list[int], partition_id: int, global_set: set[str], save_every: int = 50):\n",
    "    \"\"\"\n",
    "    Main task function to process a list of dataframe indices, classify them using the language model,\n",
    "    and save the results to a CSV file.\n",
    "    @Params:\n",
    "        index_list: List of actual dataframe indices (not positional).\n",
    "        partition_id: Identifier for the partition (used in the output file name).\n",
    "        global_set: Shared set to track already processed (job_id, skill_id) pairs.\n",
    "        save_every: Number of processed entries after which to save intermediate results.\n",
    "    \"\"\"\n",
    "    FILE_NAME = f\"../data/green_skill_classification/full_green_skills_with_GPT-4_part_{partition_id}_2.csv\"\n",
    "    CLIENT = createClient()\n",
    "\n",
    "    df_new_dataset = []\n",
    "\n",
    "    # Each thread gets its own local mappings\n",
    "    index_green_skills, id_to_skill, index_job_skills, id_to_job = get_indexes_and_mappings()\n",
    "\n",
    "    ctr = 0\n",
    "    for idx in index_list:  # now iterating over absolute dataframe indices\n",
    "        try:\n",
    "            # Attempt to reconstruct vector at index `idx`\n",
    "            D, I = index_green_skills.search(np.array([index_job_skills.reconstruct(idx)]), K_N)\n",
    "        except Exception as e:\n",
    "            send_error(f\"Searching embeddings at index {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if max(D[0]) < MIN_THRESHOLD:\n",
    "                df_new_dataset.append({\n",
    "                    \"job_id\": id_to_job[str(idx)][0],\n",
    "                    \"job_skill\": id_to_job[str(idx)][1],\n",
    "                    \"skill_id\": \"No\",\n",
    "                    \"esco_skill_name\": \"No\",\n",
    "                    \"alternative_name\": \"No\",\n",
    "                    \"prompt\": \"No\"})\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            send_error(f\"Threshold check failed at index {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "        flag = False\n",
    "        try:\n",
    "            for score, jdx in zip(D[0], I[0]):\n",
    "                try:\n",
    "                    name = df_jobs[df_jobs[\"Job_ID\"] == id_to_job[str(idx)][0]][\"Title\"].values[0]\n",
    "                    prompt = get_green_skill_classification_prompt(\n",
    "                        [f\"{id_to_skill[str(k)][0]}: {id_to_skill[str(k)][1]}\" for k in I[0]],\n",
    "                        id_to_job[str(idx)][1],\n",
    "                        name\n",
    "                    )\n",
    "                    response = run_model(prompt, CLIENT)\n",
    "                except Exception as e:\n",
    "                    send_error(f\"Cannot run model at job index {idx}, skill index {jdx}: {e}\")\n",
    "                    df_new_dataset.append({\n",
    "                        \"job_id\": id_to_job[str(idx)][0],\n",
    "                        \"job_skill\": id_to_job[str(idx)][1],\n",
    "                        \"skill_id\": \"No\",\n",
    "                        \"esco_skill_name\": \"No\",\n",
    "                        \"alternative_name\": \"No\",\n",
    "                        \"prompt\": \"No\"})\n",
    "                    break\n",
    "\n",
    "                try:\n",
    "                    if response != \"No\" and (id_to_job[str(idx)][0], id_to_skill[str(jdx)][0]) not in global_set:\n",
    "                        new_prompt = f\"{prompt} | {response}\".replace(\"\\n\", \" \").replace(\"\\t\", \" \").strip()\n",
    "                        df_new_dataset.append({\n",
    "                            \"job_id\": id_to_job[str(idx)][0],\n",
    "                            \"job_skill\": id_to_job[str(idx)][1],\n",
    "                            \"skill_id\": list(id_to_skill.keys())[jdx],\n",
    "                            \"esco_skill_name\": id_to_skill[str(jdx)][0],\n",
    "                            \"alternative_name\": id_to_skill[str(jdx)][1],\n",
    "                            \"prompt\": new_prompt})\n",
    "                        with lock:\n",
    "                            global_set.add((id_to_job[str(idx)][0], id_to_skill[str(jdx)][0]))\n",
    "                        flag = True\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    send_error(f\"Adding skill failed at job {idx}, skill {jdx}: {e}\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            send_error(f\"Iterating through scores at index {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if not flag and (id_to_job[str(idx)][1], \"No\") not in global_set:\n",
    "                new_prompt = f\"{prompt} | {response}\".replace(\"\\n\", \" \").replace(\"\\t\", \" \").strip()\n",
    "                df_new_dataset.append({\n",
    "                    \"job_id\": id_to_job[str(idx)][0],\n",
    "                    \"job_skill\": id_to_job[str(idx)][1],\n",
    "                    \"skill_id\": \"No\",\n",
    "                    \"esco_skill_name\": \"No\",\n",
    "                    \"alternative_name\": \"No\",\n",
    "                    \"prompt\": new_prompt})\n",
    "                with lock:\n",
    "                    global_set.add((id_to_job[str(idx)][0], \"No\"))\n",
    "        except Exception as e:\n",
    "            df_new_dataset.append({\n",
    "                \"job_id\": id_to_job[str(idx)][0],\n",
    "                \"job_skill\": id_to_job[str(idx)][1],\n",
    "                \"skill_id\": \"No\",\n",
    "                \"esco_skill_name\": \"No\",\n",
    "                \"alternative_name\": \"No\",\n",
    "                \"prompt\": \"No\"})\n",
    "            continue\n",
    "\n",
    "        ctr += 1\n",
    "        if ctr % save_every == 0:\n",
    "            pd.DataFrame(df_new_dataset).to_csv(FILE_NAME, index=False)\n",
    "        with lock:\n",
    "            pbar.update(1)\n",
    "\n",
    "    pd.DataFrame(df_new_dataset).to_csv(FILE_NAME, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b037e0",
   "metadata": {},
   "source": [
    "## Globar parameters for threading\n",
    "- `NUMBER_OF_PARTITIONS`: number of partitions to divide the dataset into, each partition will be processed by a different thread.\n",
    "- `TOTAL_ENTRIES`: total number of entries in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa3a1d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:   0%|          | 0/204372 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "from threading import Lock\n",
    "from tqdm import tqdm\n",
    "\n",
    "_, _, job_skills, _ = get_indexes_and_mappings()\n",
    "\n",
    "TOTAL_ENTRIES = job_skills.ntotal\n",
    "NUMBER_OF_PARTITIONS = 24\n",
    "LEFT_BOUND = 0 \n",
    "\n",
    "\"\"\"  \n",
    "Generate partition limits for dividing the dataset into multiple threads.\n",
    "@Params:\n",
    "    total_entries: Total number of entries in the dataset.\n",
    "    number_of_partitions: Number of partitions to divide the dataset into.\n",
    "    left_bound: Starting index for processing (default = 0).\n",
    "@Returns:\n",
    "    List of tuples representing the (left, right) limits for each partition.\n",
    "\"\"\"\n",
    "def generate_limits(total_entries: int, number_of_partitions: int, left_bound: int = 0):\n",
    "    effective_total = total_entries - left_bound\n",
    "    entries_per_partition = effective_total // number_of_partitions\n",
    "    partitions = [\n",
    "        (left_bound + i * entries_per_partition, left_bound + (i + 1) * entries_per_partition)\n",
    "        for i in range(number_of_partitions)\n",
    "    ]\n",
    "    partitions[-1] = (partitions[-1][0], total_entries)\n",
    "    return partitions\n",
    "\n",
    "global_set = set()\n",
    "\n",
    "partition_limits = generate_limits(TOTAL_ENTRIES, NUMBER_OF_PARTITIONS, LEFT_BOUND)\n",
    "\n",
    "pbar = tqdm(total=TOTAL_ENTRIES - LEFT_BOUND, desc=\"Overall Progress\", position=0)\n",
    "lock = Lock()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3ebb131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 8515), (8515, 17030), (17030, 25545), (25545, 34060), (34060, 42575), (42575, 51090), (51090, 59605), (59605, 68120), (68120, 76635), (76635, 85150), (85150, 93665), (93665, 102180), (102180, 110695), (110695, 119210), (119210, 127725), (127725, 136240), (136240, 144755), (144755, 153270), (153270, 161785), (161785, 170300), (170300, 178815), (178815, 187330), (187330, 195845), (195845, 204372)]\n",
      "204372\n"
     ]
    }
   ],
   "source": [
    "print(partition_limits)\n",
    "print(job_skills.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32992201",
   "metadata": {},
   "source": [
    "## Running the threading process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75eed54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:   1%|          | 2020/204372 [27:18<39:20:19,  1.43it/s] 2025-11-05 02:15 - ERROR - Iterating through scores at index 25873: [Errno 2] No such file or directory: '../data/logs/green_skill_classification/errors_part_3.log'\n",
      "2025-11-05 02:15 - ERROR - Iterating through scores at index 331: [Errno 2] No such file or directory: '../data/logs/green_skill_classification/errors_part_0.log'\n",
      "Overall Progress:   1%|          | 2022/204372 [27:21<55:49:47,  1.01it/s]2025-11-05 02:15 - ERROR - Iterating through scores at index 17373: [Errno 2] No such file or directory: '../data/logs/green_skill_classification/errors_part_2.log'\n",
      "Overall Progress:   1%|          | 2026/204372 [27:23<42:14:50,  1.33it/s]2025-11-05 02:15 - ERROR - Iterating through scores at index 8869: [Errno 2] No such file or directory: '../data/logs/green_skill_classification/errors_part_1.log'\n",
      "Overall Progress:   1%|          | 2173/204372 [29:19<23:55:24,  2.35it/s] 2025-11-05 02:17 - ERROR - Iterating through scores at index 357: [Errno 2] No such file or directory: '../data/logs/green_skill_classification/errors_part_0.log'\n",
      "Overall Progress:   2%|▏         | 4867/204372 [1:05:45<387:48:38,  7.00s/it]2025-11-05 02:53 - ERROR - Iterating through scores at index 34858: [Errno 2] No such file or directory: '../data/logs/green_skill_classification/errors_part_4.log'\n",
      "Overall Progress:  95%|█████████▌| 194782/204372 [43:37:19<3:25:54,  1.29s/it]  "
     ]
    }
   ],
   "source": [
    "threads = []\n",
    "\n",
    "MAX_THREADS = 6\n",
    "\n",
    "for i, (left, right) in enumerate(partition_limits):\n",
    "    thread = threading.Thread(target=task, args=(left, right, i, global_set))\n",
    "    threads.append(thread)\n",
    "    thread.start()\n",
    "\n",
    "    if (i + 1) % MAX_THREADS == 0 or (i + 1) == len(partition_limits):\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "        threads = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e9c04d",
   "metadata": {},
   "source": [
    "Join the results from every thread into a single dataframe and save it as a *.csv* file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee45ca6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_302021/4185461285.py:13: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  dataframe = pd.concat([dataframe, df_part], ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "dataframe = pd.DataFrame(columns=[\"job_id\", \"job_skill\", \"skill_id\" , \"month\", \"year\",\"esco_skill_name\", \"alternative_name\", \"prompt\"])\n",
    "FOLDER = \"../data/green_skill_classification\"\n",
    "\n",
    "folders = []\n",
    "\n",
    "for file in os.listdir(FOLDER):\n",
    "    if file.endswith(\".csv\"):\n",
    "        folders.append(os.path.join(FOLDER, file))\n",
    "\n",
    "folders.sort()\n",
    "for file in folders:\n",
    "    df_part = pd.read_csv(file)\n",
    "    dataframe = pd.concat([dataframe, df_part], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13a4a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv(\"../data/green_skill_classification/green_skills_with_GPT-4_full_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07024eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset shape: (204372, 6)\n",
      "Green skills dataset shape: (196019, 8)\n",
      "                   Title                Job_ID  source  \\\n",
      "0  auxiliar de mostrador  job_624fd0a8f34770fb  indeed   \n",
      "1  auxiliar de mostrador  job_624fd0a8f34770fb  indeed   \n",
      "2  auxiliar de mostrador  job_624fd0a8f34770fb  indeed   \n",
      "3  auxiliar de mostrador  job_624fd0a8f34770fb  indeed   \n",
      "4  auxiliar de mostrador  job_624fd0a8f34770fb  indeed   \n",
      "\n",
      "                                        Skills  month  year  \n",
      "0           experiencia en atencion al cliente    7.0  2024  \n",
      "1  gusto por el servicio y atencion al cliente    7.0  2024  \n",
      "2                       preparatoria terminada    7.0  2024  \n",
      "3                                mayor de anos    7.0  2024  \n",
      "4          experiencia en empleos presenciales    7.0  2024  \n",
      "                 job_id                                    job_skill skill_id  \\\n",
      "0  job_624fd0a8f34770fb           experiencia en atencion al cliente       No   \n",
      "1  job_624fd0a8f34770fb  gusto por el servicio y atencion al cliente       No   \n",
      "2  job_624fd0a8f34770fb                       preparatoria terminada       No   \n",
      "3  job_624fd0a8f34770fb                                mayor de anos       No   \n",
      "4  job_624fd0a8f34770fb          experiencia en empleos presenciales       No   \n",
      "\n",
      "   month  year esco_skill_name alternative_name  \\\n",
      "0    7.0  2024              No               No   \n",
      "1    7.0  2024              No               No   \n",
      "2    7.0  2024              No               No   \n",
      "3    7.0  2024              No               No   \n",
      "4    7.0  2024              No               No   \n",
      "\n",
      "                                              prompt  \n",
      "0  [{'role': 'system', 'content': 'You are an exp...  \n",
      "1  [{'role': 'system', 'content': 'You are an exp...  \n",
      "2  [{'role': 'system', 'content': 'You are an exp...  \n",
      "3                                                 No  \n",
      "4  [{'role': 'system', 'content': 'You are an exp...  \n"
     ]
    }
   ],
   "source": [
    "df_full = pd.read_csv(\"../data/full_dataset/jul24_to_jul_2025_cleaned_sorted.csv\")\n",
    "df_green_skills = pd.read_csv(\"../data/green_skill_classification/green_skills_with_GPT-4_full_dataset.csv\")\n",
    "\n",
    "print(\"Full dataset shape:\", df_full.shape)\n",
    "print(\"Green skills dataset shape:\", df_green_skills.shape)\n",
    "\n",
    "print(df_full.head())\n",
    "print(df_green_skills.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b9b1e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_without_no = df_green_skills[df_green_skills[\"skill_id\"] != \"No\"]\n",
    "df_full_without_no.to_csv(\"../data/green_skill_classification/green_skills_with_GPT-4_full_dataset_no_no.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fe92b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing entries shape: (8615, 6)\n"
     ]
    }
   ],
   "source": [
    "df_full = df_full.copy()\n",
    "df_full[\"__original_index__\"] = df_full.index\n",
    "\n",
    "df_merged = pd.merge(\n",
    "    df_full,\n",
    "    df_green_skills,\n",
    "    how=\"left\",\n",
    "    left_on=[\"Job_ID\", \"Skills\", \"month\", \"year\"],\n",
    "    right_on=[\"job_id\", \"job_skill\", \"month\", \"year\"],\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "df_missing = df_merged[df_merged[\"_merge\"] == \"left_only\"]\n",
    "df_missing = df_missing.set_index(\"__original_index__\")\n",
    "df_missing = df_missing[df_full.columns.drop(\"__original_index__\")]\n",
    "\n",
    "print(\"Missing entries shape:\", df_missing.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
