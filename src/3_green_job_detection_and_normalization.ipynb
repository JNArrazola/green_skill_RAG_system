{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b574dfb",
   "metadata": {},
   "source": [
    "# Green Job Detection and Normalization\n",
    "This notebooks aims to **detect** (know if a job contains green skills or not) and normalize (map the green skills from the *.csv* to the `ESCO` taxonomy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eaf3ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "import json \n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import jsonschema\n",
    "from pydantic import BaseModel\n",
    "import threading\n",
    "from threading import Lock\n",
    "\n",
    "from warning_management import send_warning\n",
    "from warning_management import send_error\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d23bc6",
   "metadata": {},
   "source": [
    "Load indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ecf7147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Green skills index contains 2539 vectors of dimension 3072.\n",
      "Job skills index contains 70318 vectors of dimension 3072.\n"
     ]
    }
   ],
   "source": [
    "index_green_skills, index_job_skills, id_to_job, id_to_skill = get_indexes_and_mappings()\n",
    "\n",
    "a, b = (index_green_skills.ntotal, index_green_skills.d,)\n",
    "c, d = (index_job_skills.ntotal, index_job_skills.d,)\n",
    "\n",
    "print(f\"Green skills index contains {a} vectors of dimension {b}.\")\n",
    "print(f\"Job skills index contains {c} vectors of dimension {d}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17cd096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_jobs = pd.read_csv(\"../data/jan_to_apr_2025_with_languages_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0166f8",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be4a63e",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "- `K_N`: number of nearest neighbors to retrieve from the index.\n",
    "- `MIN_THRESHOLD`: minimum score threshold to consider an entry relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f41441a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify this value in case of needing a different threshold for the \n",
    "# confidence of the green job detection\n",
    "MIN_THRESHOLD = 0.1\n",
    "\n",
    "# Modify this value in case of needing a different number of neighbors\n",
    "K_N = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6fa6f5",
   "metadata": {},
   "source": [
    "## Loading the indexes and mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86529ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load indexes\n",
    "\"\"\"\n",
    "@Returns:\n",
    "    index_green_skills: FAISS index for green skills embeddings.\n",
    "    index_job_skills: FAISS index for job skills embeddings.\n",
    "    id_to_job: Dictionary mapping job IDs to job names.\n",
    "    id_to_skill: Dictionary mapping skill IDs to skill names.\n",
    "\"\"\"\n",
    "def get_indexes_and_mappings() -> tuple[faiss.Index, faiss.Index, dict[str, str], dict[str, str]]:\n",
    "    index_green_skills = faiss.read_index(\"../data/embeddings/esco_green_skills_text-embedding-3-large.index\")\n",
    "    index_job_skills = faiss.read_index(\"../data/embeddings/job_skills_embeddings.index\")\n",
    "\n",
    "    # Load mappings\n",
    "    id_to_job = json.load(open(\"../data/mapping/id_to_job.json\", \"r\"))\n",
    "    id_to_skill = json.load(open(\"../data/mapping/id_to_skill.json\", \"r\"))\n",
    "    return index_green_skills, index_job_skills, id_to_job, id_to_skill\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c701f48",
   "metadata": {},
   "source": [
    "## Prompts\n",
    "## Green Skill Classification Prompt\n",
    "We will use the following prompt to detect and normalize green skills in job descriptions, it receives as arguments: \n",
    "- `k_closest`: a list of the `k` closest green skills from the `ESCO` taxonomy.\n",
    "- `skill`: the skill extracted from the job description, and also the one we want to normalize.\n",
    "- `job`: the name of the job.\n",
    "It returns a list of dictionaries representing the messages to be sent to the model and the roles of each message.\n",
    "* **system**: It is a system message that defines the role of the model.\n",
    "* **user**: It is the user message that contains the actual prompt with the context and instructions.\n",
    "* **developer**: It is the developer message that contains the instructions for the model's response format (in this case the pydantic class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56babf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_green_skill_classification_prompt(k_closest: list[str], skill: str, job: str) -> list[dict]:\n",
    "    system_prompt = \"\"\"\n",
    "You are an expert in identifying whether a skill can be mapped to a *green skill* in the ESCO taxonomy.\n",
    "A green skill is defined as the abilities, values and attitudes needed to live in, develop and support\n",
    "a society which reduces the impact of human activity on the environment.\n",
    "\n",
    "Focus on environmental or sustainability-related aspects.\n",
    "Ignore social or economic aspects unless they have a clear environmental connection.\n",
    "\"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "Determine if the following skill can be semantically matched to one of the provided green skills from ESCO.\n",
    "\n",
    "Skill to classify: \"{skill}\"\n",
    "Job context: \"{job}\"\n",
    "\n",
    "Closest ESCO green skills (formatted as 'Main Name': 'Alternative Name'):\n",
    "\"\"\"\n",
    "\n",
    "    for k in k_closest:\n",
    "        user_prompt += f\"- {k}\\n\"\n",
    "\n",
    "    user_prompt += \"\"\"\n",
    "Decide whether the skill can be reasonably mapped to one of the ESCO green skills above\n",
    "based on meaning and context.\n",
    "\"\"\"\n",
    "\n",
    "    developer_prompt = \"\"\"\n",
    "You must respond strictly following the response schema.\n",
    "If the skill matches one of the ESCO green skills, return its 'Main Name'.\n",
    "Otherwise, return 'No'.\n",
    "\"\"\"\n",
    "\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
    "        {\"role\": \"user\", \"content\": user_prompt.strip()},\n",
    "        {\"role\": \"developer\", \"content\": developer_prompt.strip()}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2e47fc",
   "metadata": {},
   "source": [
    "# Models for output parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56235158",
   "metadata": {},
   "source": [
    "## GreenSkillClassification\n",
    "Model to parse the output of the green skill normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4a0c9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreenSkillClassification(BaseModel):\n",
    "    mapped_skill: str # The main name of the mapped ESCO green skill, or \"No\" if there is no match."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5decbfa",
   "metadata": {},
   "source": [
    "## Model\n",
    "We will use the **gpt-4o** model from **OpenAI** for the normalization task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a7d7971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load OpenAI key\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_KEY\")\n",
    "\n",
    "# Source: https://platform.openai.com/docs/guides/structured-outputs\n",
    "# Function to run the model with output parsing\n",
    "def run_model(prompt: list[str], client: OpenAI) -> str:\n",
    "    response = client.responses.parse(\n",
    "        model=\"gpt-4o\",\n",
    "        input=[\n",
    "            {\"role\": msg[\"role\"], \"content\": msg[\"content\"]} for msg in prompt\n",
    "        ],\n",
    "        text_format=GreenSkillClassification,\n",
    "    )\n",
    "    return response.output_parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f70f252c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model response: mapped_skill='No'\n",
      "<class '__main__.GreenSkillClassification'>\n",
      "No\n"
     ]
    }
   ],
   "source": [
    "# Test run of the model\n",
    "D, I = index_green_skills.search(np.array([index_job_skills.reconstruct(0)]), K_N)\n",
    "k_closest = [f\"\\t{id_to_skill[str(idx)][0]}: {id_to_skill[str(idx)][1]}\" for idx in I[0]]\n",
    "name  = df_jobs[df_jobs[\"Job_ID\"] == id_to_job[str(0)][0]][\"Title\"].values[0]\n",
    "skill = id_to_job[str(0)][1]\n",
    "prompt = get_green_skill_classification_prompt(k_closest, skill, name)\n",
    "response = run_model(prompt, OpenAI(api_key=OPENAI_KEY))\n",
    "print(\"Model response:\", response)\n",
    "print(type(response))\n",
    "print(response.mapped_skill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a72fae4",
   "metadata": {},
   "source": [
    "## Prompt generator examples\n",
    "An entry used to generate sample prompts to test the prompt generator function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b23d769",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../test/example_prompts.txt\", \"w\") as f:\n",
    "    index_green_skills, index_job_skills, id_to_job, id_to_skill = get_indexes_and_mappings()\n",
    "    for i in range(0, 100):\n",
    "        D, I = index_green_skills.search(np.array([index_job_skills.reconstruct(i)]), K_N)\n",
    "    \n",
    "        k_closest = [f\"\\t{id_to_skill[str(idx)][0]}: {id_to_skill[str(idx)][1]}\" for idx in I[0]]\n",
    "        name  = df_jobs[df_jobs[\"Job_ID\"] == id_to_job[str(i)][0]][\"Title\"].values[0]\n",
    "        \n",
    "        skill = id_to_job[str(i)][1]\n",
    "        prompt = get_green_skill_classification_prompt(k_closest, skill, name)\n",
    "        f.write(str(prompt) + \"\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824f1e98",
   "metadata": {},
   "source": [
    "## Normalization process\n",
    "The following code cell shows how the normalization process is done for every job skill in the dataset. The steps are:\n",
    "1. For every i-th job, get the top `K_N` nearest neighbors to the i-th job skill, comparing them with the `ESCO` green skill taxonomy using **cosine similarity** as distance metric.\n",
    "2. If the **max score** among the `K_N` neighbors is greater or equal than `MIN_THRESHOLD`, then we consider that the job **may** contains green skills, and we proceed to normalize it using the prompt defined before and the language model.\n",
    "3. We run the prompt, were we could get two types of answers:\n",
    "   - The skill has been normalized to one of the `k` closest green skills, in that case it returns the **normalized skill**.\n",
    "   - The skill could not be normalized to any of the `k` closest green skills, in that case it returns **No**.\n",
    "4. We save the entry in a new dataframe, and also keep a set to avoid duplicates (i.e. the same job skill being normalized to the same green skill more than once, which would lead to counting it multiple times when analyzing the results)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ed8ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task(left : int, right : int, partition_id: int, global_set: set[str], save_every: int = 50):\n",
    "    # df_new_dataset = pd.DataFrame(columns=[\"job_id\", \"job_skill\", \"skill_id\" ,\"esco_skill_name\", \"alternative_name\", \"prompt\"])\n",
    "    FILE_NAME = f\"../data/green_skill_classification/green_skills_with_GPT-5_part_{partition_id}.csv\"\n",
    "    CLIENT = OpenAI(api_key = OPENAI_KEY)\n",
    "\n",
    "    df_new_dataset = []\n",
    "\n",
    "    index_green_skills, index_job_skills, id_to_job, id_to_skill = get_indexes_and_mappings()\n",
    "\n",
    "    ctr = 0  \n",
    "    for i in range(left, right): \n",
    "        try:\n",
    "            D, I = index_green_skills.search(np.array([index_job_skills.reconstruct(i)]), K_N)\n",
    "        except Exception as e:\n",
    "            send_error(f\"Searching embeddings at index {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if max(D[0]) < MIN_THRESHOLD:\n",
    "                df_new_dataset.append({\n",
    "                    \"job_id\": id_to_job[str(i)][0],\n",
    "                    \"job_skill\": id_to_job[str(i)][1],\n",
    "                    \"skill_id\": \"No\",\n",
    "                    \"esco_skill_name\": \"No\",\n",
    "                    \"alternative_name\": \"No\",\n",
    "                    \"prompt\": \"No\"})\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            send_error(f\"Threshold check or concatenation failed at index {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        flag = False\n",
    "        try:\n",
    "            for score, idx in zip(D[0], I[0]):\n",
    "                try:\n",
    "                    name  = df_jobs[df_jobs[\"Job_ID\"] == id_to_job[str(i)][0]][\"Title\"].values[0]\n",
    "                    prompt = get_green_skill_classification_prompt([f\"{id_to_skill[str(idx)][0]}: {id_to_skill[str(idx)][1]}\" for idx in I[0]], id_to_job[str(i)][1], name)\n",
    "                    response = run_model(prompt, CLIENT)\n",
    "\n",
    "                    response.mapped_skill.replace(\"//\", \"\").strip()\n",
    "                except Exception as e:\n",
    "                    df_new_dataset.append({\n",
    "                        \"job_id\": id_to_job[str(i)][0],\n",
    "                        \"job_skill\": id_to_job[str(i)][1],\n",
    "                        \"skill_id\": \"No\",\n",
    "                        \"esco_skill_name\": \"No\",\n",
    "                        \"alternative_name\": \"No\",\n",
    "                        \"prompt\": \"No\"})\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    if response.mapped_skill != \"No\" and (id_to_job[str(i)][0], id_to_skill[str(idx)][0]) not in global_set:\n",
    "                        new_prompt = str(prompt) + \" | \" + response.mapped_skill\n",
    "                        new_prompt = new_prompt.replace(\"\\n\", \" \").replace(\"\\t\", \" \").strip()\n",
    "\n",
    "                        df_new_dataset.append({\n",
    "                            \"job_id\": id_to_job[str(i)][0],\n",
    "                            \"job_skill\": id_to_job[str(i)][1],\n",
    "                            \"skill_id\": list(id_to_skill.keys())[idx],\n",
    "                            \"esco_skill_name\": id_to_skill[str(idx)][0],\n",
    "                            \"alternative_name\": id_to_skill[str(idx)][1],\n",
    "                            \"prompt\": new_prompt})\n",
    "                        with lock:\n",
    "                            global_set.add((id_to_job[str(i)][0], id_to_skill[str(idx)][0]))\n",
    "                        flag = True\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    send_error(f\"Adding green skill at job index {i}, skill index {idx}: {e}\")\n",
    "                    continue\n",
    "        except Exception as e:\n",
    "            send_error(f\"Iterating through scores at index {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if not flag and (id_to_job[str(i)][1], \"No\") not in global_set:\n",
    "                df_new_dataset.append({\n",
    "                    \"job_id\": id_to_job[str(i)][0],\n",
    "                    \"job_skill\": id_to_job[str(i)][1],\n",
    "                    \"skill_id\": \"No\",\n",
    "                    \"esco_skill_name\": \"No\",\n",
    "                    \"alternative_name\": \"No\",\n",
    "                    \"prompt\": \"No\"})\n",
    "                with lock:\n",
    "                    global_set.add((id_to_job[str(i)][0], \"No\"))\n",
    "        except Exception as e:\n",
    "\n",
    "            df_new_dataset.append({\n",
    "                    \"job_id\": id_to_job[str(i)][0],\n",
    "                    \"job_skill\": id_to_job[str(i)][1],\n",
    "                    \"skill_id\": \"No\",\n",
    "                    \"esco_skill_name\": \"No\",\n",
    "                    \"alternative_name\": \"No\",\n",
    "                    \"prompt\": \"No\"})\n",
    "            continue\n",
    "        ctr += 1\n",
    "        if ctr % save_every == 0:\n",
    "            pd.DataFrame(df_new_dataset).to_csv(FILE_NAME, index=False)\n",
    "        with lock:\n",
    "            pbar.update(1)\n",
    "    pd.DataFrame(df_new_dataset).to_csv(FILE_NAME, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3a1d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress:   0%|          | 0/70318 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Global parameters for partitioning\n",
    "def generate_limits(total_entries: int, number_of_partitions: int):\n",
    "    entries_per_partition = total_entries // number_of_partitions\n",
    "    partitions = [(i * entries_per_partition, (i + 1) * entries_per_partition) for i in range(number_of_partitions)]\n",
    "    partitions[-1] = (partitions[-1][0], total_entries)\n",
    "    return partitions\n",
    "\n",
    "global_set = set()\n",
    "NUMBER_OF_PARTITIONS = 10\n",
    "\n",
    "_, job_skills, _, _ = get_indexes_and_mappings()\n",
    "\n",
    "TOTAL_ENTRIES = job_skills.ntotal\n",
    "partition_limits = generate_limits(TOTAL_ENTRIES, NUMBER_OF_PARTITIONS)\n",
    "\n",
    "pbar = tqdm(total=job_skills.ntotal, desc=\"Overall Progress\", position=0)\n",
    "lock = Lock()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3ebb131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 7031), (7031, 14062), (14062, 21093), (21093, 28124), (28124, 35155), (35155, 42186), (42186, 49217), (49217, 56248), (56248, 63279), (63279, 70318)]\n"
     ]
    }
   ],
   "source": [
    "print(partition_limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75eed54d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Progress: 100%|█████████▉| 70245/70318 [11:58:56<04:06,  3.38s/it]  "
     ]
    }
   ],
   "source": [
    "threads = []\n",
    "\n",
    "for partition_id, (left, right) in enumerate(partition_limits):\n",
    "    thread = threading.Thread(target=task, args=(left, right, partition_id, global_set))\n",
    "    threads.append(thread)\n",
    "    thread.start()\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee45ca6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame(columns=[\"job_id\", \"job_skill\", \"skill_id\" ,\"esco_skill_name\", \"alternative_name\", \"prompt\"])\n",
    "FOLDER = \"../data/green_skill_classification/\"\n",
    "\n",
    "folders = []\n",
    "\n",
    "for file in os.listdir(FOLDER):\n",
    "    if file.endswith(\".csv\"):\n",
    "        folders.append(os.path.join(FOLDER, file))\n",
    "\n",
    "folders.sort()\n",
    "for file in folders:\n",
    "    df_part = pd.read_csv(file)\n",
    "    dataframe = pd.concat([dataframe, df_part], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13a4a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.to_csv(\"../data/green_skill_classification/green_skills_with_GPT-5_full_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f3567071",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_dataset.to_csv(\"../data/green_skills_with_GPT-5.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
